{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e527f340",
   "metadata": {},
   "source": [
    "### Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47323b67",
   "metadata": {},
   "source": [
    "#### Document Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbd5f968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b87d40ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(\n",
    "    page_content=\"this is the main text content I am using to create RAG\",\n",
    "    metadata = {\n",
    "        \"source\": \"example.txt\",\n",
    "        \"pages\": 1,\n",
    "        \"author\": \"Sushmita\",\n",
    "        \"date_created\": \"2026-02-16\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ccb34890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.txt', 'pages': 1, 'author': 'Sushmita', 'date_created': '2026-02-16'}, page_content='this is the main text content I am using to create RAG')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ebc373",
   "metadata": {},
   "source": [
    "#### Create a simple txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a95bbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"../data/text_files\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9cc0cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample file created\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "   \"../data/text_files/python.txt\" : '''Python is a high-level, interpreted programming language known for its simplicity and readability. Created by Guido van Rossum and first released in 1991, Python emphasizes clean syntax and developer productivity.\n",
    "\n",
    "It is one of the most popular programming languages in the world and is widely used in:\n",
    "\n",
    "Web development\n",
    "\n",
    "Data science\n",
    "\n",
    "Artificial intelligence\n",
    "\n",
    "Automation\n",
    "\n",
    "Cybersecurity\n",
    "\n",
    "Software development\n",
    "\n",
    "Game development'''\n",
    "}\n",
    "\n",
    "for filepath, content in sample_texts.items():\n",
    "    with open(filepath, 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "        \n",
    "print(\"Sample file created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99175af",
   "metadata": {},
   "source": [
    "### TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1fc791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"../data/text_files/python.txt\", encoding=\"utf-8\")\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4403d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python.txt'}, page_content='Python is a high-level, interpreted programming language known for its simplicity and readability. Created by Guido van Rossum and first released in 1991, Python emphasizes clean syntax and developer productivity.\\n\\nIt is one of the most popular programming languages in the world and is widely used in:\\n\\nWeb development\\n\\nData science\\n\\nArtificial intelligence\\n\\nAutomation\\n\\nCybersecurity\\n\\nSoftware development\\n\\nGame development')]\n"
     ]
    }
   ],
   "source": [
    "print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc444558",
   "metadata": {},
   "source": [
    "### Directory Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14203b28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\python.txt'}, page_content='Python is a high-level, interpreted programming language known for its simplicity and readability. Created by Guido van Rossum and first released in 1991, Python emphasizes clean syntax and developer productivity.\\n\\nIt is one of the most popular programming languages in the world and is widely used in:\\n\\nWeb development\\n\\nData science\\n\\nArtificial intelligence\\n\\nAutomation\\n\\nCybersecurity\\n\\nSoftware development\\n\\nGame development')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "# load all the text files from the directory\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/text_files\",\n",
    "    glob=\"**/*.txt\", # Pattern to match the files\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={'encoding': 'utf-8'},\n",
    "    show_progress=False\n",
    "    \n",
    ")\n",
    "\n",
    "documents = dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce33ba26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2026-01-28T19:29:27+05:45', 'source': '..\\\\data\\\\pdf_files\\\\SushmitaMalakar_CV.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\SushmitaMalakar_CV.pdf', 'total_pages': 2, 'format': 'PDF 1.7', 'title': '', 'author': 'LENOVO', 'subject': '', 'keywords': '', 'moddate': '2026-01-28T19:29:27+05:45', 'trapped': '', 'modDate': \"D:20260128192927+05'45'\", 'creationDate': \"D:20260128192927+05'45'\", 'page': 0}, page_content='SUSHMITA MALAKAR \\nDATA SCIENCE ENTHUSIAST \\n9818085057 | sushmalakar10@gmail.com | Satungal, Kathmandu \\nwww.linkedin.com/in/sushmita-malakar-a3a5a9247 \\nwww.github.com/sushmitamalakar10 \\n \\n \\n \\n \\nABOUT ME \\nI am passionate and motivated in Data Science. I have completed hands-on projects using Python and \\nbasic machine learning techniques. I am confident in data cleaning, exploration and visualization. I am \\neager to apply my skills and continue learning through real-world experience. I am especially interested in \\ngaining practical knowledge by working on meaningful projects in a collaborative environment. \\n \\nSKILLS \\nLanguages & Tools Python, SQL, Pandas, NumPy, Matplotlib, Seaborn, Scikit-learn, Jupyter Notebook, \\nGoogle Colab, GitHub, Flask \\nData Skills \\nData Cleaning, EDA, Regression, Classification, Data Visualization, Data Analysis \\nSoft Skills \\n \\n \\nPROJECTS \\nMenstrual Cycle Predictor \\n• Built ML model to predict days left until the next period and the next-period date using 17,976 records \\nand inputs such as last period date, previous cycle length, mood/stress, sleep, pain level, flow level \\nand PMS symptoms. \\n• Compared multiple models (Linear Regression, Decision Tree, Random Forest, XGBoost) with an 80/20 \\ntrain–test split, and measured results using MAE, RMSE, and R². \\n• Improved performance by tuning XGBoost, reaching MAE 1.44 days, RMSE 1.99 days, R² 0.31. \\n• Saved the best trained model with Joblib and integrated it into Streamlit for prediction. \\nCustomer Churn Prediction \\n• Built a telecom customer churn classifier on 7,043 records using 19 processed features; cleaned data, \\nencoded categorical variables, and handled imbalance with SMOTE. \\n• Trained and compared tree/ensemble models, achieving best results with a tuned Random Forest: test \\naccuracy 0.784, churn class F1-score 0.61 (also evaluated tuned Gradient Boosting: accuracy 0.773, \\nchurn F1 0.63; tuned Decision Tree: accuracy 0.734, churn F1 0.57). \\nCERTIFICATIONS \\nPython Data Science Fundamentals \\nwww.udemy.com/certificate/UC-75aa8feb-5aa2-4f41-a8dc-f36b8d38b167/ \\n2025 \\nPython Numpy Data Analysis for Data Scientist | AI | ML | DL \\nwww.udemy.com/certificate/UC-75b790ac-66fc-4524-9ba6-28a2ecdb845c/ \\n2025 \\n \\nEDUCATION'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2026-01-28T19:29:27+05:45', 'source': '..\\\\data\\\\pdf_files\\\\SushmitaMalakar_CV.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\SushmitaMalakar_CV.pdf', 'total_pages': 2, 'format': 'PDF 1.7', 'title': '', 'author': 'LENOVO', 'subject': '', 'keywords': '', 'moddate': '2026-01-28T19:29:27+05:45', 'trapped': '', 'modDate': \"D:20260128192927+05'45'\", 'creationDate': \"D:20260128192927+05'45'\", 'page': 1}, page_content='Bachelor of Computer Application, Tribhuvan University \\n \\nDivya Gyan College, Putalisadak, Kathmandu \\n2025')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "\n",
    "dir_loader = DirectoryLoader(\n",
    "    \"../data/pdf_files\",\n",
    "    glob = \"**/*.pdf\",\n",
    "    loader_cls = PyMuPDFLoader,\n",
    "    show_progress = False\n",
    ")\n",
    "\n",
    "pdf_documents = dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "846e6537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pdf_documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a14eb85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 PDF files to process\n",
      "\n",
      "Processing: SushmitaMalakar_CV.pdf\n",
      "  ✓ Loaded 2 pages\n",
      "\n",
      "Total documents loaded: 2\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1879e8d9",
   "metadata": {},
   "source": [
    "### Text splitting into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7bfad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Text splitting get into chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c408e6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 2 documents into 4 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: SUSHMITA MALAKAR  \n",
      "DATA SCIENCE ENTHUSIAST  \n",
      "9818085057 | sushmalakar10@gmail.com | Satungal, Kathmandu \n",
      "www.linkedin.com/in/sushmita-malakar-a3a5a9247 \n",
      "www.github.com/sushmitamalakar10   \n",
      " \n",
      " \n",
      "ABOUT M...\n",
      "Metadata: {'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2026-01-28T19:29:27+05:45', 'author': 'LENOVO', 'moddate': '2026-01-28T19:29:27+05:45', 'source': '..\\\\data\\\\pdf_files\\\\SushmitaMalakar_CV.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'SushmitaMalakar_CV.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2026-01-28T19:29:27+05:45', 'author': 'LENOVO', 'moddate': '2026-01-28T19:29:27+05:45', 'source': '..\\\\data\\\\pdf_files\\\\SushmitaMalakar_CV.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'SushmitaMalakar_CV.pdf', 'file_type': 'pdf'}, page_content='SUSHMITA MALAKAR  \\nDATA SCIENCE ENTHUSIAST  \\n9818085057 | sushmalakar10@gmail.com | Satungal, Kathmandu \\nwww.linkedin.com/in/sushmita-malakar-a3a5a9247 \\nwww.github.com/sushmitamalakar10   \\n \\n \\nABOUT ME \\nI am passionate and motivated in Data Science. I have completed hands -on projects using Python and \\nbasic machine learning techniques. I am confident in data cleaning, exploration and visualization. I am \\neager to apply my skills and continue learning through real-world experience. I am especially interested in \\ngaining practical knowledge by working on meaningful projects in a collaborative environment. \\n \\nSKILLS \\nLanguages & Tools Python, SQL, Pandas, NumPy, Matplotlib, Seaborn, Scikit -learn, Jupyter Notebook, \\nGoogle Colab, GitHub, Flask \\nData Skills Data Cleaning, EDA, Regression, Classification, Data Visualization, Data Analysis \\nSoft Skills  \\n \\nPROJECTS \\nMenstrual Cycle Predictor'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2026-01-28T19:29:27+05:45', 'author': 'LENOVO', 'moddate': '2026-01-28T19:29:27+05:45', 'source': '..\\\\data\\\\pdf_files\\\\SushmitaMalakar_CV.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'SushmitaMalakar_CV.pdf', 'file_type': 'pdf'}, page_content='Google Colab, GitHub, Flask \\nData Skills Data Cleaning, EDA, Regression, Classification, Data Visualization, Data Analysis \\nSoft Skills  \\n \\nPROJECTS \\nMenstrual Cycle Predictor \\n• Built ML model to predict days left until the next period and the next-period date using 17,976 records \\nand inputs such as last period date, previous cycle length, mood/stre ss, sleep, pain level, flow level \\nand PMS symptoms. \\n• Compared multiple models (Linear Regression, Decision Tree, Random Forest, XGBoost) with an 80/20 \\ntrain–test split, and measured results using MAE, RMSE, and R². \\n• Improved performance by tuning XGBoost, reaching MAE 1.44 days, RMSE 1.99 days, R² 0.31. \\n• Saved the best trained model with Joblib and integrated it into Streamlit for prediction. \\nCustomer Churn Prediction \\n• Built a telecom customer churn classifier on 7,043 records using 19 processed features; cleaned data, \\nencoded categorical variables, and handled imbalance with SMOTE.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2026-01-28T19:29:27+05:45', 'author': 'LENOVO', 'moddate': '2026-01-28T19:29:27+05:45', 'source': '..\\\\data\\\\pdf_files\\\\SushmitaMalakar_CV.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'SushmitaMalakar_CV.pdf', 'file_type': 'pdf'}, page_content='Customer Churn Prediction \\n• Built a telecom customer churn classifier on 7,043 records using 19 processed features; cleaned data, \\nencoded categorical variables, and handled imbalance with SMOTE. \\n• Trained and compared tree/ensemble models, achieving best results with a tuned Random Forest: test \\naccuracy 0.784, churn class F1 -score 0.61 (also evaluated tuned Gradient Boosting: accuracy 0.773, \\nchurn F1 0.63; tuned Decision Tree: accuracy 0.734, churn F1 0.57). \\nCERTIFICATIONS \\nPython Data Science Fundamentals \\nwww.udemy.com/certificate/UC-75aa8feb-5aa2-4f41-a8dc-f36b8d38b167/ 2025 \\nPython Numpy Data Analysis for Data Scientist | AI | ML | DL \\nwww.udemy.com/certificate/UC-75b790ac-66fc-4524-9ba6-28a2ecdb845c/ 2025 \\n \\nEDUCATION'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2026-01-28T19:29:27+05:45', 'author': 'LENOVO', 'moddate': '2026-01-28T19:29:27+05:45', 'source': '..\\\\data\\\\pdf_files\\\\SushmitaMalakar_CV.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2', 'source_file': 'SushmitaMalakar_CV.pdf', 'file_type': 'pdf'}, page_content='Bachelor of Computer Application, Tribhuvan University  \\nDivya Gyan College, Putalisadak, Kathmandu 2025')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6504a777",
   "metadata": {},
   "source": [
    "### Embedding and vectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5703497b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea92ac3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\CV\\InfoDevelopers\\Internship\\Day9\\RAG-pipeline\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\LENOVO\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 112.62it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x2bcb17d4310>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using SentenceTransformer\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize the embedding manager\n",
    "        \n",
    "        Args: \n",
    "            model_name: HuggingFace model name for sentence Embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "        \n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the SentenceTransformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "        \n",
    "    \n",
    "    def generate_embeddings(self, texts: List[str]) -> np.array:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args: \n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts),embedding_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embeddings for {len(texts)} texts..\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "    def get_embedding_dimension(self) -> int:\n",
    "        \"\"\"Get the embedding dimension of the model\"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        return self.model.get_sentence_embedding_dimension()\n",
    "    \n",
    "\n",
    "# Initalize embedding manager\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc77589",
   "metadata": {},
   "source": [
    "### Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6368ed5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x2bcb2353050>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import uuid\n",
    "from typing import List, Any\n",
    "\n",
    "import numpy as np\n",
    "import chromadb\n",
    "\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"Manage document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        collection_name: str = \"pdf_documents\",\n",
    "        persist_directory: str = \"../data/vector_store\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the vector store.\n",
    "\n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection.\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"},\n",
    "            )\n",
    "\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store.\n",
    "\n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "\n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "\n",
    "        # Prepare the data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata[\"doc_index\"] = i\n",
    "            metadata[\"content_length\"] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "\n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text,\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "vectorstore = VectorStore()\n",
    "vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f94740ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VectorStore:\n",
    "#     \"\"\"Manage document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "#     def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "#         \"\"\"\n",
    "#         Initalize the vector store\n",
    "        \n",
    "#         Args:\n",
    "#         collection_name: Name of the ChromaDB Collection\n",
    "#         persist_directory: Directory to persist the vector store\n",
    "#         \"\"\"\n",
    "        \n",
    "#         self.collection_name = collection_name\n",
    "#         self.persist_directory = persist_directory\n",
    "#         self.client = None\n",
    "#         self.collection = None\n",
    "#         self._initialize_store()\n",
    "        \n",
    "\n",
    "#     def _initialize_store(self):\n",
    "#         \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "#         try: \n",
    "#             #create persistent ChromaDB client\n",
    "#             os.makedirs(self.persist_directory, exist_ok=True)\n",
    "#             self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "#             # Get or create collection\n",
    "#             self.collection = self.client.get_or_create_collection(\n",
    "#                 name = self.collection_name,\n",
    "#                 metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "#             )\n",
    "            \n",
    "#             print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "#             print(f\"Existing documents in colleciton: {self.collection.count()}\")\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error initializing vector store: {e}\")\n",
    "\n",
    "\n",
    "#     def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "#         \"\"\"\n",
    "#         Add documents and their embeddings to the vector store\n",
    "        \n",
    "#         Args:\n",
    "#             documents: list of LangChain documents\n",
    "#             embeddings: Corresponding embeddings for the documents\n",
    "#         \"\"\"\n",
    "        \n",
    "#         if len(documents) != len(embeddings):\n",
    "#             raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "#         print(f\"Adding {len(documents)} documents to vetor store...\")\n",
    "            \n",
    "            \n",
    "#         # Prepare the data for ChromaDB\n",
    "            \n",
    "#         ids = []\n",
    "#         metadatas = []\n",
    "#         documents_text = []\n",
    "#         embeddings_list = []\n",
    "        \n",
    "#         for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "#             # Generate unique ID\n",
    "#             doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "#             ids.append(doc_id)\n",
    "\n",
    "#             # Prepare metadata\n",
    "#             metadata = dict(doc.metadata)\n",
    "#             metadata['doc_index']\n",
    "#             metadata['content_length'] = len(doc.page_content)\n",
    "#             metadatas.append(metadata)\n",
    "            \n",
    "#             # Document content\n",
    "#             documents_text.append(doc.page_content)\n",
    "            \n",
    "#             # Embedding\n",
    "#             embeddings_list.append(embedding.tolist()) \n",
    "            \n",
    "#             # Add to collection\n",
    "#         try:\n",
    "#             self.colleciton.add(\n",
    "#                 ids=ids,\n",
    "#                 embeddings=embeddings_list,\n",
    "#                 metadatas = metadata,\n",
    "#                 documents=documents_text\n",
    "#             )\n",
    "#             print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "#             print(f\"Total documents in colleciton: {self.collection.count()}\")\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error adding documents to vector store: {e}\")\n",
    "#             raise\n",
    "        \n",
    "        \n",
    "# vectorstore = VectorStore()\n",
    "# vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3afc43f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2026-01-28T19:29:27+05:45', 'author': 'LENOVO', 'moddate': '2026-01-28T19:29:27+05:45', 'source': '..\\\\data\\\\pdf_files\\\\SushmitaMalakar_CV.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'SushmitaMalakar_CV.pdf', 'file_type': 'pdf'}, page_content='SUSHMITA MALAKAR  \\nDATA SCIENCE ENTHUSIAST  \\n9818085057 | sushmalakar10@gmail.com | Satungal, Kathmandu \\nwww.linkedin.com/in/sushmita-malakar-a3a5a9247 \\nwww.github.com/sushmitamalakar10   \\n \\n \\nABOUT ME \\nI am passionate and motivated in Data Science. I have completed hands -on projects using Python and \\nbasic machine learning techniques. I am confident in data cleaning, exploration and visualization. I am \\neager to apply my skills and continue learning through real-world experience. I am especially interested in \\ngaining practical knowledge by working on meaningful projects in a collaborative environment. \\n \\nSKILLS \\nLanguages & Tools Python, SQL, Pandas, NumPy, Matplotlib, Seaborn, Scikit -learn, Jupyter Notebook, \\nGoogle Colab, GitHub, Flask \\nData Skills Data Cleaning, EDA, Regression, Classification, Data Visualization, Data Analysis \\nSoft Skills  \\n \\nPROJECTS \\nMenstrual Cycle Predictor'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2026-01-28T19:29:27+05:45', 'author': 'LENOVO', 'moddate': '2026-01-28T19:29:27+05:45', 'source': '..\\\\data\\\\pdf_files\\\\SushmitaMalakar_CV.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'SushmitaMalakar_CV.pdf', 'file_type': 'pdf'}, page_content='Google Colab, GitHub, Flask \\nData Skills Data Cleaning, EDA, Regression, Classification, Data Visualization, Data Analysis \\nSoft Skills  \\n \\nPROJECTS \\nMenstrual Cycle Predictor \\n• Built ML model to predict days left until the next period and the next-period date using 17,976 records \\nand inputs such as last period date, previous cycle length, mood/stre ss, sleep, pain level, flow level \\nand PMS symptoms. \\n• Compared multiple models (Linear Regression, Decision Tree, Random Forest, XGBoost) with an 80/20 \\ntrain–test split, and measured results using MAE, RMSE, and R². \\n• Improved performance by tuning XGBoost, reaching MAE 1.44 days, RMSE 1.99 days, R² 0.31. \\n• Saved the best trained model with Joblib and integrated it into Streamlit for prediction. \\nCustomer Churn Prediction \\n• Built a telecom customer churn classifier on 7,043 records using 19 processed features; cleaned data, \\nencoded categorical variables, and handled imbalance with SMOTE.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2026-01-28T19:29:27+05:45', 'author': 'LENOVO', 'moddate': '2026-01-28T19:29:27+05:45', 'source': '..\\\\data\\\\pdf_files\\\\SushmitaMalakar_CV.pdf', 'total_pages': 2, 'page': 0, 'page_label': '1', 'source_file': 'SushmitaMalakar_CV.pdf', 'file_type': 'pdf'}, page_content='Customer Churn Prediction \\n• Built a telecom customer churn classifier on 7,043 records using 19 processed features; cleaned data, \\nencoded categorical variables, and handled imbalance with SMOTE. \\n• Trained and compared tree/ensemble models, achieving best results with a tuned Random Forest: test \\naccuracy 0.784, churn class F1 -score 0.61 (also evaluated tuned Gradient Boosting: accuracy 0.773, \\nchurn F1 0.63; tuned Decision Tree: accuracy 0.734, churn F1 0.57). \\nCERTIFICATIONS \\nPython Data Science Fundamentals \\nwww.udemy.com/certificate/UC-75aa8feb-5aa2-4f41-a8dc-f36b8d38b167/ 2025 \\nPython Numpy Data Analysis for Data Scientist | AI | ML | DL \\nwww.udemy.com/certificate/UC-75b790ac-66fc-4524-9ba6-28a2ecdb845c/ 2025 \\n \\nEDUCATION'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2021', 'creator': 'Microsoft® Word 2021', 'creationdate': '2026-01-28T19:29:27+05:45', 'author': 'LENOVO', 'moddate': '2026-01-28T19:29:27+05:45', 'source': '..\\\\data\\\\pdf_files\\\\SushmitaMalakar_CV.pdf', 'total_pages': 2, 'page': 1, 'page_label': '2', 'source_file': 'SushmitaMalakar_CV.pdf', 'file_type': 'pdf'}, page_content='Bachelor of Computer Application, Tribhuvan University  \\nDivya Gyan College, Putalisadak, Kathmandu 2025')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e6d11d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SUSHMITA MALAKAR  \\nDATA SCIENCE ENTHUSIAST  \\n9818085057 | sushmalakar10@gmail.com | Satungal, Kathmandu \\nwww.linkedin.com/in/sushmita-malakar-a3a5a9247 \\nwww.github.com/sushmitamalakar10   \\n \\n \\nABOUT ME \\nI am passionate and motivated in Data Science. I have completed hands -on projects using Python and \\nbasic machine learning techniques. I am confident in data cleaning, exploration and visualization. I am \\neager to apply my skills and continue learning through real-world experience. I am especially interested in \\ngaining practical knowledge by working on meaningful projects in a collaborative environment. \\n \\nSKILLS \\nLanguages & Tools Python, SQL, Pandas, NumPy, Matplotlib, Seaborn, Scikit -learn, Jupyter Notebook, \\nGoogle Colab, GitHub, Flask \\nData Skills Data Cleaning, EDA, Regression, Classification, Data Visualization, Data Analysis \\nSoft Skills  \\n \\nPROJECTS \\nMenstrual Cycle Predictor',\n",
       " 'Google Colab, GitHub, Flask \\nData Skills Data Cleaning, EDA, Regression, Classification, Data Visualization, Data Analysis \\nSoft Skills  \\n \\nPROJECTS \\nMenstrual Cycle Predictor \\n• Built ML model to predict days left until the next period and the next-period date using 17,976 records \\nand inputs such as last period date, previous cycle length, mood/stre ss, sleep, pain level, flow level \\nand PMS symptoms. \\n• Compared multiple models (Linear Regression, Decision Tree, Random Forest, XGBoost) with an 80/20 \\ntrain–test split, and measured results using MAE, RMSE, and R². \\n• Improved performance by tuning XGBoost, reaching MAE 1.44 days, RMSE 1.99 days, R² 0.31. \\n• Saved the best trained model with Joblib and integrated it into Streamlit for prediction. \\nCustomer Churn Prediction \\n• Built a telecom customer churn classifier on 7,043 records using 19 processed features; cleaned data, \\nencoded categorical variables, and handled imbalance with SMOTE.',\n",
       " 'Customer Churn Prediction \\n• Built a telecom customer churn classifier on 7,043 records using 19 processed features; cleaned data, \\nencoded categorical variables, and handled imbalance with SMOTE. \\n• Trained and compared tree/ensemble models, achieving best results with a tuned Random Forest: test \\naccuracy 0.784, churn class F1 -score 0.61 (also evaluated tuned Gradient Boosting: accuracy 0.773, \\nchurn F1 0.63; tuned Decision Tree: accuracy 0.734, churn F1 0.57). \\nCERTIFICATIONS \\nPython Data Science Fundamentals \\nwww.udemy.com/certificate/UC-75aa8feb-5aa2-4f41-a8dc-f36b8d38b167/ 2025 \\nPython Numpy Data Analysis for Data Scientist | AI | ML | DL \\nwww.udemy.com/certificate/UC-75b790ac-66fc-4524-9ba6-28a2ecdb845c/ 2025 \\n \\nEDUCATION',\n",
       " 'Bachelor of Computer Application, Tribhuvan University  \\nDivya Gyan College, Putalisadak, Kathmandu 2025']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the text to embeddings\n",
    "\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7f3c2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 4 texts..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (4, 384)\n",
      "Adding 4 documents to vector store...\n",
      "Successfully added 4 documents to vector store\n",
      "Total documents in collection: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate the embeddings\n",
    "embbeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "# Store in the vector database\n",
    "vectorstore.add_documents(chunks, embbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a453510e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01196015, -0.06975466, -0.00867063, ...,  0.03594098,\n",
       "        -0.06827877,  0.00370967],\n",
       "       [-0.13038181, -0.10577191, -0.00522372, ..., -0.07579198,\n",
       "        -0.04352811,  0.02119745],\n",
       "       [-0.09743939, -0.05643107, -0.03003179, ..., -0.07283021,\n",
       "        -0.0385066 , -0.00064882],\n",
       "       [-0.03035072,  0.03633566, -0.00208108, ..., -0.03320826,\n",
       "        -0.08560763,  0.01523414]], shape=(4, 384), dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5554fed5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAG-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
